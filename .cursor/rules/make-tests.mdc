---
description: Create comprehensive tests for code changes in a GitHub Pull Request or git branch. Use this rule when you need to write tests for new or modified code following BDD patterns and project conventions.
globs:
  - 'src/**/*.tsx'
  - 'src/**/*.ts'
---

# Make Tests Guide

This guide helps create comprehensive tests for code changes in a GitHub Pull Request or git branch, following the established patterns and conventions in this codebase.

## Usage

Invoke this rule with a PR number or branch name:

### Using PR Number

```
/make-tests #123
/make-tests 123
```

### Using Branch Name

```
/make-tests feature/my-new-feature
/make-tests origin/fix/bug-123
```

## Input Detection

Determine the input type:

1. **PR Number**: If the argument is numeric or starts with `#` followed by numbers (e.g., `#123`, `123`)
2. **Branch Name**: If the argument contains letters, slashes, or dashes (e.g., `feature/my-feature`, `origin/fix-bug`)

## Overview

This guide will help you:

1. Detect input type (PR number or branch name)
2. Analyze the PR or branch to identify changed/added files (compared to `main`)
3. **Create a coverage map** accounting for every new file (tested or explicitly excluded)
4. Create tests following BDD approach (GIVEN/WHEN/THEN) for **all non-excluded files**
5. Target **minimum 80% coverage** on new code (higher for hooks and utils)
6. Reuse existing factories/mocks and refactor shared ones

## Critical Testing Philosophy

**Test by default.** Every new file with logic MUST have a corresponding test file. The default is to write tests — skipping a file requires explicit justification.

### The 80% Target is a Minimum, Not a Ceiling

- **80% coverage on new code** is the minimum target for files with testable logic
- Aim higher (90%+) for business-critical code (payments, invoices, forms, hooks)
- Coverage below 80% requires explicit justification in the coverage map (Phase 2)
- **Every new `.tsx` or `.ts` file with >20 lines of code MUST have a test file**, unless it falls into the explicit exclusion list below

### Default: Write Tests

Write tests for ALL new code, including:

- Business logic with conditional paths
- State management or side effects
- User interactions (forms, buttons, navigation)
- Data transformations or calculations
- Error handling
- Edge cases
- Components with conditional rendering
- Hooks (always — they have a contract to verify)
- Utility functions (always — they have inputs/outputs to verify)
- Form components (validation, submission, field interactions)
- Components that receive props and render differently based on them

### How to Determine If a File Has Testable Logic

**A file has testable logic if it contains ANY of the following. Even ONE match = MUST test:**

- `useState`, `useEffect`, `useLayoutEffect`, `useMemo`, `useCallback`
- `useNavigate`, `useParams`, `useSearchParams` or any routing hook
- Any custom hook call (`use*`)
- GraphQL queries or mutations (`useQuery`, `useLazyQuery`, `useMutation`)
- Conditional rendering (`{condition && ...}`, ternary in JSX)
- Event handlers (`onClick`, `onChange`, `onSubmit`)
- `if`/`else`, `switch`, ternary operators in logic
- `.map()`, `.filter()`, `.reduce()` on data
- Toast notifications, clipboard operations, dialog/modal management
- Any callback passed to child components

**A file is "pure presentational" ONLY if it literally does nothing but return static JSX with props interpolation — no hooks, no conditions, no handlers, no state. This is extremely rare in practice.**

### Explicit Exclusions (The ONLY Cases Where You Skip Tests)

You may skip tests ONLY for files that are:

- Pure type definitions or interfaces (`.d.ts`, type-only files)
- Configuration or constants with no logic
- Index/barrel export files
- Generated files (GraphQL codegen, etc.)
- Translation files
- CSS/style-only files

**Any file NOT in this list MUST have tests.** When in doubt, write the test.

### When Skipping a Test, Justify It

If you decide not to test a file, you MUST document WHY in the coverage map (Phase 2). Valid reasons:

- "Pure type definitions, no runtime behavior"
- "Barrel export, no logic"
- "Generated file"

Invalid reasons (do NOT use these to skip tests):

- "Simple component" — if it has ANY hook, state, or conditional rendering, it needs tests
- "Thin wrapper" — if it adds any logic, transforms data, or calls hooks, it needs tests
- "Only renders props" — if it has any conditional rendering or event handlers, it needs tests
- "Low complexity" — complexity is not the only reason to test; correctness is
- "Mostly presentational" — if it uses hooks, manages state, or has click handlers, it is NOT presentational
- "Page component" — page components orchestrate logic and ALWAYS need tests
- "Just renders a table/list" — if it has column configs, actions, or row handlers, it needs tests

---

## Prerequisites

Before starting, gather context by reading these reference files:

1. **Testing Best Practices**: `agents/testing-practices.md`
2. **Code Quality Standards**: `agents/code-quality.md`
3. **Existing Test Example**: `src/components/invoices/details/__tests__/InvoiceDetailsTable.integration.test.tsx`
4. **Test Utils**: `src/test-utils.tsx`

---

## Phase 0: Parse Input

When invoked, detect the input type:

**PR Number:**
- `/make-tests #123` → PR_NUMBER = `123`
- `/make-tests 123` → PR_NUMBER = `123`

**Branch Name:**
- `/make-tests feature/my-feature` → BRANCH = `feature/my-feature`
- `/make-tests origin/fix/bug-123` → BRANCH = `origin/fix/bug-123`

If no argument is provided, ask the user what they want to create tests for.

---

## Phase 1: Code Analysis

### Step 1.1: Fetch Changed Files Information

Based on the input type, fetch the diff and changed files:

#### Option A: PR Number

```bash
# Get PR diff and changed files
gh pr view <PR_NUMBER> --json files,additions,deletions,body,title
gh pr diff <PR_NUMBER>
```

#### Option B: Branch Name

```bash
# For local branch - get the diff against main
git diff main...<BRANCH_NAME> --name-only  # List changed files
git diff main...<BRANCH_NAME>               # Full diff

# If the branch is remote (origin/branch-name)
git fetch origin
git diff main...origin/<BRANCH_NAME> --name-only
git diff main...origin/<BRANCH_NAME>

# If you're currently ON the branch you want to test
git diff main...HEAD --name-only
git diff main...HEAD
```

### Step 1.2: Identify Files Requiring Tests

**Default: every new/changed file with >20 lines of code gets a test file.** Only the explicit exclusions below are exempt.

**Files that ALWAYS need tests (non-negotiable):**

- Components (`.tsx` files in `src/components/`, `src/pages/`) — even "simple" ones if they have any logic
- Hooks (`.ts` files in `src/hooks/` or custom hooks anywhere) — always, no exceptions
- Utilities (`.ts` files with functions) — always, no exceptions
- Validation schemas — always (test valid/invalid inputs)
- Form components — always (test validation, submission, interactions)
- Business logic modules — always

**Files excluded from testing:**

- Type definitions (`.d.ts`, pure type-only files with no runtime code)
- Generated files (`src/generated/`)
- Translation files (`.json`)
- Style files (`.css`, `.scss`)
- Index/barrel exports (files that only re-export)
- Constants-only files with no logic (pure static data)

### Step 1.3: Quantify Test Scope

Count the files that need tests and confirm the scope:

```markdown
PR Analysis:
- Total files changed: X
- Files needing tests: Y (list them)
- Files excluded: Z (list them with reason)
- Expected test files to create: Y
```

**If the ratio of "files needing tests" to "total non-excluded files" is below 70%, you are being too conservative. Re-evaluate.**

### Step 1.4: Check for Implicit Coverage and Related Files

Before creating tests, verify that ALL new files in the PR are accounted for:

**1. Supporting Files (schemas, configs, utils)**

- Validation schemas and utility files should ALWAYS have their own test files — they have clear inputs/outputs
- Config files with no logic can be tested implicitly through component tests
- If a supporting file has exported functions, it needs its own tests

**2. Custom Hooks (ALWAYS test)**

- Hooks MUST always have their own test file — no exceptions
- Test their **core contract**: what they return and what side effects they trigger
- Focus on: callbacks return expected values, correct parameters passed to external dependencies
- Don't skip hook tests just because the hook is "simple" - if it has a contract, test it

**3. New Patterns Introduced by the PR**

- Identify if the PR introduces new approaches or replaces old patterns
- Focus tests specifically on the NEW behavior, not on unchanged existing logic
- If a PR changes HOW something is done (e.g., navigation, state management), test that the new approach works correctly

---

## Phase 2: Test Planning

### Step 2.1: Coverage Map (MANDATORY)

**Before writing any test, you MUST create a coverage map that accounts for EVERY new or significantly changed file in the PR.** This ensures no file is accidentally skipped.

```markdown
## Coverage Map

| # | File | Lines | Test File | Status | Skip Reason |
|---|------|-------|-----------|--------|-------------|
| 1 | `src/pages/WebhookForm.tsx` | 327 | `__tests__/WebhookForm.test.tsx` | ✅ WILL TEST | — |
| 2 | `src/hooks/useWebhookEventTypes.ts` | 211 | `__tests__/useWebhookEventTypes.test.ts` | ✅ WILL TEST | — |
| 3 | `src/hooks/useDeleteWebhook.ts` | 50 | `__tests__/useDeleteWebhook.test.ts` | ✅ WILL TEST | — |
| 4 | `src/types/webhook.ts` | 15 | — | ⏭️ SKIP | Pure type definitions |
| 5 | `src/generated/graphql.tsx` | 267 | — | ⏭️ SKIP | Generated file |
```

**Rules for the coverage map:**

1. **Every file** with >20 lines of additions MUST appear in the map
2. Files marked ✅ WILL TEST must get a test file — no exceptions
3. Files marked ⏭️ SKIP must have a valid skip reason from the Explicit Exclusions list
4. **Hooks ALWAYS get tested** — no skip allowed
5. **Utility files ALWAYS get tested** — no skip allowed
6. **Page/form components ALWAYS get tested** — no skip allowed
7. If the map shows more than 30% of files being skipped, reconsider — you are likely being too conservative

### Step 2.2: User Checkpoint (MANDATORY — STOP HERE)

**⛔ STOP: Before writing any test, you MUST present the coverage map to the user and ask for confirmation.**

Present the table from Step 2.1 to the user and ask:

> "Here is the coverage map for this PR. I will create test files for all ✅ items and skip ⏭️ items for the reasons listed. Do you want me to proceed, or would you like to adjust any decisions?"

**Do NOT proceed to Phase 3 until the user confirms.** The user may:

- Ask you to test a file you planned to skip
- Ask you to skip a file you planned to test
- Ask you to adjust the scope

### Step 2.3: Code Review Per File

For each file marked ✅ WILL TEST, briefly identify key test scenarios:

```markdown
### File: `src/path/to/Component.tsx`

**Key scenarios to test:**
- Default rendering with required props
- Conditional rendering paths (lines X-Y)
- User interactions (form submit, button clicks)
- Error/loading states
- Edge cases (empty data, invalid input)
```

Keep this brief — the goal is to plan, not to over-analyze. If a file has logic, test it.

### Step 2.4: Search for Existing Mocks and Factories

Before creating new mocks, search for existing ones:

```bash
# Search for existing factories
find src -name "*factory*" -o -name "*Factory*" | head -20

# Search for existing mocks
find src -name "*mock*" -o -name "*Mock*" | head -20

# Search for shared test utilities
ls -la src/__mocks__/ 2>/dev/null || echo "No shared mocks folder"
```

---

## Phase 3: Implementation

### Step 3.1: Add data-test Constants to Components

**CRITICAL:** Before writing tests, add `data-test` constants to the component being tested.

**In the component file:**

```typescript
// Export data-test constants at the top of the component file (after imports)
export const COMPONENT_NAME_TEST_ID = 'component-name'
export const COMPONENT_NAME_TITLE_TEST_ID = 'component-name-title'
export const COMPONENT_NAME_SUBMIT_BUTTON_TEST_ID = 'component-name-submit-button'
export const COMPONENT_NAME_ERROR_MESSAGE_TEST_ID = 'component-name-error-message'
// Add more as needed for testable elements

export const ComponentName = ({ ... }) => {
  return (
    <div data-test={COMPONENT_NAME_TEST_ID}>
      <Typography data-test={COMPONENT_NAME_TITLE_TEST_ID}>
        {translate('...')}
      </Typography>
      {/* For form.SubmitButton use dataTest (camelCase) */}
      <form.SubmitButton dataTest={COMPONENT_NAME_SUBMIT_BUTTON_TEST_ID}>
        Submit
      </form.SubmitButton>
    </div>
  )
}
```

**Naming convention:**

- Use SCREAMING_SNAKE_CASE for constant names
- Use kebab-case for the actual data-test value
- Pattern: `{COMPONENT_NAME}_{ELEMENT_DESCRIPTION}_TEST_ID`

### Step 3.2: Create Test File

Create test file at: `src/path/to/__tests__/ComponentName.test.tsx`

**Test file structure:**

```typescript
import { screen, waitFor } from '@testing-library/react'
import userEvent from '@testing-library/user-event'

import {
  COMPONENT_NAME_TEST_ID,
  COMPONENT_NAME_TITLE_TEST_ID,
  COMPONENT_NAME_SUBMIT_BUTTON_TEST_ID,
  ComponentName,
} from '../ComponentName'
import { render } from '~/test-utils'

// Mock dependencies
jest.mock('~/hooks/core/useInternationalization', () => ({
  useInternationalization: () => ({
    translate: (key: string) => key,
  }),
}))

describe('ComponentName', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })

  afterEach(() => {
    jest.restoreAllMocks()
  })

  describe('GIVEN the component is rendered', () => {
    describe('WHEN in default state', () => {
      it('THEN should display the main container', () => {
        render(<ComponentName />)

        expect(screen.getByTestId(COMPONENT_NAME_TEST_ID)).toBeInTheDocument()
      })
    })
  })
})
```

### Step 3.3: BDD Test Structure Rules

**MANDATORY:** All test descriptions MUST follow this pattern:

1. **describe** blocks use `GIVEN` or `WHEN` (always UPPERCASE)
2. **it** blocks use `THEN` (always UPPERCASE)
3. The rest of the description is lowercase

**Pattern:**

```typescript
describe('GIVEN [precondition]', () => {
  describe('WHEN [action or state]', () => {
    it('THEN should [expected outcome]', () => {
      // test implementation
    })
  })
})
```

**Examples:**

```typescript
// Correct
describe('GIVEN the user is logged in', () => {
  describe('WHEN clicking the logout button', () => {
    it('THEN should redirect to login page', () => { ... })
    it('THEN should clear the session', () => { ... })
  })
})

// Incorrect - don't do this
describe('Given the user is logged in', () => { ... })  // Wrong case
describe('When clicking the logout button', () => { ... })  // Wrong case
it('Then should redirect', () => { ... })  // Wrong case
it('should redirect', () => { ... })  // Missing THEN
```

### Step 3.4: Using it.each for Similar Tests

**IMPORTANT:** When you have multiple tests that follow the same pattern but with different inputs/outputs, use `it.each` to reduce code duplication and improve maintainability.

**When to use `it.each`:**

- Testing multiple elements are rendered (e.g., form fields, buttons)
- Testing multiple inputs produce expected outputs
- Testing the same behavior with different props/states
- Testing multiple validation cases

**Pattern for checking multiple elements are displayed:**

```typescript
describe('WHEN the component renders', () => {
  it.each([
    ['name input field', COMPONENT_NAME_INPUT_TEST_ID],
    ['email input field', COMPONENT_EMAIL_INPUT_TEST_ID],
    ['cancel button', COMPONENT_CANCEL_BUTTON_TEST_ID],
    ['submit button', COMPONENT_SUBMIT_BUTTON_TEST_ID],
  ])('THEN should display the %s', (_, testId) => {
    render(<ComponentName />)

    expect(screen.getByTestId(testId)).toBeInTheDocument()
  })
})
```

**Pattern for checking default values:**

```typescript
describe('WHEN form fields are empty', () => {
  it.each([
    ['name', COMPONENT_NAME_INPUT_TEST_ID],
    ['email', COMPONENT_EMAIL_INPUT_TEST_ID],
    ['phone', COMPONENT_PHONE_INPUT_TEST_ID],
  ])('THEN should have empty %s input by default', (_, testId) => {
    render(<ComponentName />)

    const container = screen.getByTestId(testId)
    const input = container.querySelector('input')

    expect(input).toHaveValue('')
  })
})
```

**When NOT to use `it.each`:**

- When tests have significantly different setup or assertions
- When the test logic is complex and would be harder to read in a table
- When you only have 1-2 similar tests (not worth the abstraction)
- When debugging would be harder due to the abstraction

### Step 3.5: Selector Rules

**NEVER use translation keys as selectors:**

```typescript
// WRONG - Never do this
expect(screen.getByText('text_17440321235444hcxi31f8j6')).toBeInTheDocument()
expect(screen.getByText(translate('some_key'))).toBeInTheDocument()

// CORRECT - Use data-test IDs
expect(screen.getByTestId(COMPONENT_NAME_TITLE_TEST_ID)).toBeInTheDocument()
```

**For form inputs, use the data-test container:**

```typescript
// CORRECT - Find input within data-test container
const inputContainer = screen.getByTestId(COMPONENT_NAME_INPUT_TEST_ID)
const input = inputContainer.querySelector('input') as HTMLInputElement
await user.type(input, 'test value')
```

### Step 3.6: Reuse and Refactor Mocks

**Step 3.6.1: Check for existing mocks**

Before creating new mocks, search the codebase for similar mocks in existing test files.

**Step 3.6.2: Refactor shared mocks**

If you find the same mock used in multiple test files, move it to a shared location:

1. Create shared mock files in `src/__mocks__/` or `src/test-utils/mocks/`
2. Export factory functions for creating mock objects
3. Update existing tests to import from the shared location

---

## Phase 4: Coverage Analysis (Critical Evaluation)

### Step 4.1: Run Tests with Coverage on New Files Only

```bash
# Run coverage ONLY on the new/changed files from the PR
pnpm test:coverage -- --collectCoverageFrom='src/path/to/new-file.tsx' src/path/to/__tests__/new-file.test.tsx
```

### Step 4.2: Analyze Uncovered Code

When reviewing coverage results, for each uncovered line/branch:

1. **Default: write a test for it.** Uncovered code is a gap that should be filled.
2. **You may skip uncovered lines ONLY if** they are:
   - Pure JSX markup with no conditions
   - Logging/console statements
   - TypeScript type narrowing that has no runtime effect
3. **Always add tests for uncovered:**
   - Conditional branches (if/else, ternary, switch)
   - Error handling paths
   - Callback functions
   - State transitions

### Step 4.3: Coverage Targets

**Minimum target: 80% on new code.** Higher coverage for critical paths.

| Scenario                         | Target Coverage | Minimum Coverage | Notes                                      |
| -------------------------------- | --------------- | ---------------- | ------------------------------------------ |
| Business logic / utils           | 90-100%         | 80%              | Must cover all branches and edge cases     |
| Hooks                            | 90-100%         | 80%              | Must cover contract, callbacks, side effects|
| Form with validation             | 80-95%          | 70%              | Must cover validation, submission, errors  |
| Component with conditional logic | 80-90%          | 70%              | Must cover all rendering paths             |
| Component with minor logic       | 70-85%          | 60%              | Must cover interactions and state changes  |
| Pure presentational, no logic    | 0%              | 0%               | Only files in Explicit Exclusions list     |

**IMPORTANT**: "Pure presentational, no logic" means the component ONLY renders static JSX with no conditionals, no event handlers, no state, no hooks. If it has ANY of those, it is NOT pure presentational and MUST be tested.

### Step 4.4: Keep Test Files Clean

**IMPORTANT:** Do NOT add coverage note comments to test files. Test files should contain only tests, mocks, and necessary setup code.

```typescript
// DON'T DO THIS
/**
 * Coverage Note: This test file achieves ~65% coverage...
 * Uncovered code includes: ...
 */
```

If coverage is below the minimum targets in Step 4.3, add more tests to close the gap before moving on.

### Step 4.5: Run All Tests

```bash
pnpm test src/path/to/__tests__/file.test.tsx
```

---

## Phase 5: Final Checklist

### Test Quality Checklist

- [ ] **Coverage map completed** - Every new file is accounted for (tested or explicitly excluded)
- [ ] **All files marked ✅ in coverage map have test files** — no exceptions
- [ ] Tests follow BDD structure (GIVEN/WHEN/THEN in UPPERCASE)
- [ ] **it.each used** where appropriate for similar tests
- [ ] **⛔ NO TRANSLATION KEYS USED AS SELECTORS** - All selectors use data-test IDs
- [ ] **⛔ NO TRANSLATION KEYS IN ASSERTIONS** - Use `expect.objectContaining()` for partial matching, use `expect.any(String)` for title/description
- [ ] data-test constants are exported from the component
- [ ] Tests import data-test constants from the component
- [ ] Existing mocks/factories are reused
- [ ] Shared mocks are extracted to shared mock folder (src/\_\_mocks\_\_/) if used in multiple files
- [ ] **Coverage meets minimum targets** (see Step 4.3 table)
- [ ] Tests pass: `pnpm test <test-file>`
- [ ] Linting passes: `pnpm lint <test-file>`

### ⛔ MANDATORY: Translation Key Verification (Final Step)

**Before completing the task, you MUST verify that NO translation keys exist in the test files.**

Run this check on all test files created or modified:

```bash
# Search for translation key patterns in test files (text_ followed by alphanumeric characters)
grep -E "text_[a-zA-Z0-9]+" src/path/to/__tests__/*.test.tsx
```

**If ANY translation keys are found, you MUST remove them:**

1. Replace `title: 'text_xxx'` with `title: expect.any(String)` or remove entirely
2. Replace `description: 'text_xxx'` with `description: expect.any(String)` or remove entirely
3. Replace `actionText: 'text_xxx'` with `actionText: expect.any(String)` or remove entirely
4. Replace `message: 'text_xxx'` with removal (only keep `severity` for toasts)
5. Replace `screen.getByText('text_xxx')` with `screen.getByTestId(COMPONENT_TEST_ID)`

**This check is NON-NEGOTIABLE. Tests with translation keys will break when translations change.**

### Coverage Decision Guide

| New Code Type                       | Action                                    | Min Coverage |
| ----------------------------------- | ----------------------------------------- | ------------ |
| Hooks (custom)                      | **Always test** — contract, returns, side effects | 80%          |
| Utility functions                   | **Always test** — inputs, outputs, edge cases     | 80%          |
| Form pages / components             | **Always test** — validation, submission, errors   | 70%          |
| Business logic with branches        | **Always test** — all paths and edge cases         | 80%          |
| Component with conditional rendering| **Always test** — all rendering paths              | 70%          |
| Component with user interactions    | **Always test** — clicks, navigation, state        | 70%          |
| Validation schemas                  | **Always test** — valid/invalid inputs             | 80%          |
| Pure type definitions               | Skip — no runtime behavior                         | 0%           |
| Generated files                     | Skip — auto-generated                              | 0%           |
| Constants / config (no logic)       | Skip — static data                                 | 0%           |
| Barrel exports                      | Skip — no logic                                    | 0%           |

### What NOT to Test Within a File

These are things to skip **within** a file you ARE testing (not reasons to skip the entire file):

- Translation key values (dynamic and can change)
- Pure UI styling (colors, fonts, spacing)
- Third-party library internals
- Loading skeleton JSX (but DO test the loading state toggle)
- Basic JSX structure without conditions

---

## Common Patterns

### Testing Loading States

```typescript
describe('GIVEN the component is loading', () => {
  describe('WHEN data is being fetched', () => {
    it('THEN should display loading skeleton', () => {
      render(<Component isLoading={true} />)

      expect(screen.getByTestId(COMPONENT_LOADING_SKELETON_TEST_ID)).toBeInTheDocument()
    })
  })
})
```

### Testing Error States

```typescript
describe('GIVEN an error occurred', () => {
  describe('WHEN the error is displayed', () => {
    it('THEN should show error message', () => {
      render(<Component error="Something went wrong" />)

      expect(screen.getByTestId(COMPONENT_ERROR_TEST_ID)).toBeInTheDocument()
    })
  })
})
```

### Testing Form Submissions

```typescript
describe('GIVEN the form is filled', () => {
  describe('WHEN user submits the form', () => {
    it('THEN should call the submit handler with form values', async () => {
      const onSubmit = jest.fn()
      const user = userEvent.setup()
      render(<FormComponent onSubmit={onSubmit} />)

      const nameInput = screen.getByTestId(FORM_NAME_INPUT_TEST_ID).querySelector('input') as HTMLInputElement
      await user.type(nameInput, 'Test Name')

      const submitButton = screen.getByTestId(FORM_SUBMIT_BUTTON_TEST_ID)
      await user.click(submitButton)

      await waitFor(() => {
        expect(onSubmit).toHaveBeenCalledWith(
          expect.objectContaining({ name: 'Test Name' })
        )
      })
    })
  })
})
```

### Testing Dialog/Modal Opens (CRITICAL: No Translation Keys)

When testing that a dialog or modal was opened with specific properties, **NEVER** include translation keys for `title`, `description`, or `actionText`. Instead, verify only non-translation properties like `colorVariant`, or use `expect.any(String)` if you need to verify the presence of text fields.

```typescript
describe('GIVEN user wants to delete an item', () => {
  describe('WHEN clicking the delete button', () => {
    it('THEN should open confirmation dialog with danger variant', async () => {
      const user = userEvent.setup()
      render(<Component />)

      const deleteButton = screen.getByTestId(COMPONENT_DELETE_BUTTON_TEST_ID)
      await user.click(deleteButton)

      // ✅ CORRECT - Only verify non-translation key properties
      expect(mockDialogOpen).toHaveBeenCalledWith(
        expect.objectContaining({
          colorVariant: 'danger',
        }),
      )

      // ✅ ALSO CORRECT - If you need to verify title/description exist, use expect.any(String)
      expect(mockDialogOpen).toHaveBeenCalledWith(
        expect.objectContaining({
          title: expect.any(String),
          description: expect.any(String),
          colorVariant: 'danger',
        }),
      )

      // ❌ WRONG - NEVER include translation keys in dialog assertions
      // expect(mockDialogOpen).toHaveBeenCalledWith(
      //   expect.objectContaining({
      //     title: 'text_6271200984178801ba8bdeb2',        // ❌ NEVER DO THIS
      //     description: 'text_6271200984178801ba8bded2',  // ❌ NEVER DO THIS
      //     actionText: 'text_6271200984178801ba8bdf0c',   // ❌ NEVER DO THIS
      //     colorVariant: 'danger',
      //   }),
      // )
    })
  })
})
```

### Testing Conditional Rendering

```typescript
describe('GIVEN the feature flag is enabled', () => {
  describe('WHEN component renders', () => {
    it('THEN should display the new feature', () => {
      render(<Component featureEnabled={true} />)

      expect(screen.getByTestId(COMPONENT_NEW_FEATURE_TEST_ID)).toBeInTheDocument()
    })
  })
})

describe('GIVEN the feature flag is disabled', () => {
  describe('WHEN component renders', () => {
    it('THEN should not display the new feature', () => {
      render(<Component featureEnabled={false} />)

      expect(screen.queryByTestId(COMPONENT_NEW_FEATURE_TEST_ID)).not.toBeInTheDocument()
    })
  })
})
```

### Testing with Timezone (for date components)

```typescript
import { Settings } from 'luxon'

describe('ComponentWithDates', () => {
  const originalDefaultZone = Settings.defaultZone

  beforeAll(() => {
    Settings.defaultZone = 'UTC'
  })

  afterAll(() => {
    Settings.defaultZone = originalDefaultZone
  })

  // ... tests
})
```
